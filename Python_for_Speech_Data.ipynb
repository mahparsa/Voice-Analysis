{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZY6yl0Zm6Ec"
      },
      "source": [
        "An introduction to Speech Signal Processing in Python:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_b2pmrQnDHx"
      },
      "source": [
        "\n",
        "# 1. Audio Datasets\n",
        "## 1.1.[ESC-50: Dataset for Environmental Sound Classification](https://github.com/karolpiczak/ESC-50)\n",
        "### 1.1 `Notebook`\n",
        "[Simple training tutorial](https://colab.research.google.com/github/fastaudio/fastaudio/blob/master/docs/ESC50:%20Environmental%20Sound%20Classification.ipynb)\n",
        "\n",
        "## 1.2. https://www2.cs.uic.edu/~i101/SoundFiles/\n",
        "\n",
        "## 1.3. https://urbansounddataset.weebly.com/download-urbansound8k.html\n",
        "\n",
        "## 1.4 https://www.epidemicsound.com/sound-effects/body/?_us=adwords&_usx=11367246386_&gclid=CjwKCAjw49qKBhAoEiwAHQVTo9Vr6EwFqyz1nFx9dpnddQTWMdNd4DVVlMndtOJun4LCowkcZnBKaRoCB5kQAvD_BwE\n",
        "\n",
        "## 1.5 https://www2.cs.uic.edu/~i101/SoundFiles/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmY5I7Hfg9WN"
      },
      "source": [
        "Dataset\n",
        "http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/8kHz_16bit/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfeJ664XoXLr"
      },
      "source": [
        "## LibROSA\n",
        "LibROSA is a python library to process audio data.\n",
        "LibROSA can be used to:\n",
        "1. Extract acoustic features\n",
        "2. Create  speech spectrogram\n",
        "3. Create Fourier Transformation of speech signals\n",
        "\n",
        "Creating spectrograms is a method to make speech signals obvious. A spectrogram is a readout that displays frequency on the vertical axis, time on the horizontal axis, and amplitude (i.e., amount of sound energy) as either darkness or coloration. A spectrogram is described as a heat map. A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_USX5JYpiQO"
      },
      "source": [
        "## Installing Librosa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8pfLhIfphUm",
        "outputId": "d3c84ef2-7d24-4952-e8a6-e56c82a50331"
      },
      "source": [
        "pip install librosa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (2.4.7)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ay9JALvpynH"
      },
      "source": [
        "Using librosa you can import different types of audio codecs and .wav files. To read any audio file, you would just need to pass the file_path to librosa.load() function.\n",
        "This function returns:\n",
        "1. An array of amplitudes.\n",
        "2. Sampling rate (the sampling rate refers to ‘sampling frequency’ used\n",
        "\n",
        "While recording the audio file, please note:\n",
        "\n",
        "\n",
        "1. if the argument sr = None, the function will load your audio file in its original sampling rate.\n",
        "2. You can specify the custom sampling rate as per your requirement, the function can upsample or downsample the signal for you)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ZvDPOgqD4q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6AIEUYpqJb1"
      },
      "source": [
        "!wget -nc https://www2.cs.uic.edu/~i101/SoundFiles/StarWars3.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kESIm_XuqZIx"
      },
      "source": [
        "import librosa\n",
        "x , sr = librosa.load('StarWars3.wav', sr=None)\n",
        "# x is a a numpy array.\n",
        "# sr is the sampling rate of the audio file, by default it is equal to 22KHZ\n",
        "print(x)\n",
        "print(sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefukZOvqgBY"
      },
      "source": [
        "## Visualizing Audio:\n",
        "\n",
        "You can plot the audio array using **librosa.display.waveplot:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4w6VbiRqj7c"
      },
      "source": [
        "\n",
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wtXgjB6qnGY"
      },
      "source": [
        "x , sr = librosa.load('StarWars3.wav', sr=44000)\n",
        "#We can change this behavior by resampling at sr=44.1KHz. On the other hand, x is a digitized audio signal that has a specified frequency and sample rate.\n",
        "\n",
        "print(x)\n",
        "print(sr)\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6gEr9etqqWR"
      },
      "source": [
        "## Playing an audio signal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1lIfbPHqv1l"
      },
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio('StarWars3.wav')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgV2HhTqq13z"
      },
      "source": [
        "import numpy as np\n",
        "sr = 22050\n",
        "# choose the sample rate e.g., 44000\n",
        "T = 5.0    # seconds\n",
        "t = np.linspace(0, T, int(T*sr), endpoint=False)\n",
        "x = 0.5*np.sin(2*np.pi*220*t)\n",
        "\n",
        "\n",
        "#playing generated audio\n",
        "ipd.Audio(x, rate=sr) # load a NumPy array\n",
        "\n",
        "import soundfile as sf\n",
        "sf.write('example.wav', x, sr)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DxJHKnRq6sC"
      },
      "source": [
        "# Pre-processing methods for audio signals\n",
        "\n",
        "## Normalization\n",
        "Using normalization techniques we can adjust the volume of audio signals to a standard set level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFjH2_NHrDeO"
      },
      "source": [
        "!pip install scikit-learn\n",
        "!sudo apt-get install build-essential swig\n",
        "!pip install auto-sklearn==0.11.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC0VO3NOrIXY"
      },
      "source": [
        "import sklearn\n",
        "#min = minimum value for each row of the vector signal\n",
        "#max = maximum value for each row of the vector signal\n",
        "def normalize(x, axis=0):\n",
        "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
        "\n",
        "#Plotting the Spectral Centroid along the waveform\n",
        "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
        "librosa.display.waveplot(normalize(x), sr=sr, alpha=0.2,color='r')\n",
        "\n",
        "#plt.plot(normalize(x), color='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzszOGC5rMLH"
      },
      "source": [
        "## 3.2. Pre-emphasis\n",
        "Pre-emphasis is a technique that should be done before exteracting features. This technique can boost the signal’s high-frequency components. Thus, it dose not change the low-frequency components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAxqU0b2rRwz"
      },
      "source": [
        "!wget -nc https://www.soundsnap.com/male_auctioneer_pa_voice_introducing_and_selling_sgi_server\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLhBLWuwrWNo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y, sr = librosa.load('male_auctioneer_pa_voice_introducing_and_selling_sgi_server', offset=30, duration=1)\n",
        "y_filt = librosa.effects.preemphasis(y)\n",
        "# and plot the results for comparison\n",
        "S_orig = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "S_preemph = librosa.amplitude_to_db(np.abs(librosa.stft(y_filt)), ref=np.max)\n",
        "\n",
        "librosa.display.specshow(S_orig, y_axis='log', x_axis='time')\n",
        "plt.title('Original signal')\n",
        "librosa.display.specshow(S_preemph, y_axis='log', x_axis='time')\n",
        "plt.title('Pre-emphasized signal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMpihVNNqV7f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifJ0Q044hRaY"
      },
      "source": [
        "Working with Speech Signals:\n",
        "# Framing\n",
        "\n",
        "A speech signal is a non-stationary signal ( a signal with variable frequency contents). So, to analyze a speech signal, we need to view it as a stationary signal. To do so, we divide speech signals into short frames. Generally speaking, we choose small intervals (e.g., about 20 to 30 milliseconds) to make the frame. Note that the shape of the human vocal tract will be constant for such short periods. Suppose we choose an interval shorter than 20 milliseconds. In that case, we won't have sufficient samples to obtain a reasonable estimate of signals frequency components, while in a break longer than 30 milliseconds, the signal's frequency components may fluctuate too much, so the frame can not be considered as a stationary signal [3-4].\n",
        "\n",
        "**bold text**### A Script for creating frams from an audio *signals*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZTWbfum5cQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBXhrrDHhap5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        " def framing(sig, fs=16000, win_len=0.025, win_hop=0.01):\n",
        "     \"\"\"\n",
        "     transform a signal into a series of overlapping frames.\n",
        "\n",
        "     Args:\n",
        "         sig            (array) : a mono audio signal (Nx1) from which to compute features.\n",
        "         fs               (int) : the sampling frequency of the signal we are working with.\n",
        "                                  Default is 16000.\n",
        "         win_len        (float) : window length in sec.\n",
        "                                  Default is 0.025.\n",
        "         win_hop        (float) : step between successive windows in sec.\n",
        "                                  Default is 0.01.\n",
        "\n",
        "     Returns:\n",
        "         array of frames.\n",
        "         frame length.\n",
        "     \"\"\"\n",
        "     # compute frame length and frame step (convert from seconds to samples)\n",
        "     frame_length = win_len * fs\n",
        "     frame_step = win_hop * fs\n",
        "     signal_length = len(sig)\n",
        "     frames_overlap = frame_length - frame_step\n",
        "\n",
        "     # Make sure that we have at least 1 frame+\n",
        "     num_frames = np.abs(signal_length - frames_overlap) // np.abs(frame_length - frames_overlap)\n",
        "     rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)\n",
        "\n",
        "     # Pad Signal to make sure that all frames have equal number of samples\n",
        "     # without truncating any samples from the original signal\n",
        "     if rest_samples != 0:\n",
        "         pad_signal_length = int(frame_step - rest_samples)\n",
        "         z = np.zeros((pad_signal_length))\n",
        "         pad_signal = np.append(sig, z)\n",
        "         num_frames += 1\n",
        "     else:\n",
        "         pad_signal = sig\n",
        "\n",
        "     # make sure to use integers as indices\n",
        "     frame_length = int(frame_length)\n",
        "     frame_step = int(frame_step)\n",
        "     num_frames = int(num_frames)\n",
        "\n",
        "     # compute indices\n",
        "     idx1 = np.tile(np.arange(0, frame_length), (num_frames, 1))\n",
        "     idx2 = np.tile(np.arange(0, num_frames * frame_step, frame_step),\n",
        "                    (frame_length, 1)).T\n",
        "     indices = idx1 + idx2\n",
        "     frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "     return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAIL8tKshk2M"
      },
      "source": [
        "# Windowing\n",
        "Note that when we extract frames from a speech signal, we extract a set of waveforms with a non-integer number of periods. It leads to an issue known as spectral leakage in signal processing lingo and means the signal cannot correctly represent its frequency components. To address this issue, we use windowing to split frames into several waveforms that go to zero at the borders [4-5]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMCs9r9Vhnxu"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        " def windowing(frames, frame_len, win_type=\"hamming\", beta=14):\n",
        "     \"\"\"\n",
        "     generate and apply a window function to avoid spectral leakage.\n",
        "\n",
        "     Args:\n",
        "       frames  (array) : array including the overlapping frames.\n",
        "       frame_len (int) : frame length.\n",
        "       win_type  (str) : type of window to use.\n",
        "                         Default is \"hamming\"\n",
        "\n",
        "     Returns:\n",
        "       windowed frames.\n",
        "     \"\"\"\n",
        "     if   win_type == \"hamming\" : windows = np.hamming(frame_len)\n",
        "     elif win_type == \"hanning\" : windows = np.hanning(frame_len)\n",
        "     elif win_type == \"bartlet\" : windows = np.bartlett(frame_len)\n",
        "     elif win_type == \"kaiser\"  : windows = np.kaiser(frame_len, beta)\n",
        "     elif win_type == \"blackman\": windows = np.blackman(frame_len)\n",
        "     windowed_frames = frames * windows\n",
        "     return windowed_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5I53H8IhrIs"
      },
      "source": [
        "Overlapping frames\n",
        "Windowing leads to losing the samples towards the beginning and the end of the frame and finally to an incorrect frequency representation. To address the issue, we take overlapping frames instead of disjoint frames. The overlap between frames is generally accepted to be of 10-15 ms.\n",
        "The code below is a modified version of a script at [6]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLqK1WBphuQu"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile   #This library is used for reading the .wav file\n",
        "[fs,signal]=scipy.io.wavfile.read(‘w1.wav’) #input wav file ,change here\n",
        "# fs=sampling frequency,signal is the numpy 2D array where the data of the wav file is written\n",
        "length=len(signal) # the length of the wav file.This gives the number of samples ,not the length in time\n",
        "window_hop_length=0.01 #10ms change here\n",
        "overlap=int(fs*window_hop_length)\n",
        "print (”overlap=” ,overlap)\n",
        "window_size=0.025 #25 ms,change here\n",
        "framesize=int(window_size*fs)\n",
        "print “framesize=”,framesize\n",
        "number_of_frames=(length/overlap);\n",
        "nfft_length=framesize #length of DFT ,change here\n",
        "print “number of frames are =”,number_of_frames\n",
        "frames=numpy.ndarray((number_of_frames,framesize)) # This declares a 2D matrix,with rows equal to the number of frames,and columns equal to the framesize or the length of each DTF\n",
        "for k in range(0,number_of_frames):\n",
        "for i in range(0,framesize):\n",
        "    if((k*overlap+i)<length):\n",
        "      frames[k][i]=signal[k*overlap+i]\n",
        "   else:\n",
        "      frames[k][i]=0\n",
        "fft_matrix=numpy.ndarray((number_of_frames,framesize)) #declares another 2d matrix to store  the DFT of each windowed frame\n",
        "abs_fft_matrix=numpy.ndarray((number_of_frames,framesize)) #declares another 2D Matrix to store the power spectrum\n",
        "for k in range(0,number_of_frames):\n",
        "fft_matrix[k]=numpy.fft.fft(frames[k]) #computes the DFT\n",
        "abs_fft_matrix[k]=abs(fft_matrix[k])*abs(fft_matrix[k])/(max(abs(fft_matrix[k]))) # computes the power spectrum\n",
        "t=range(len(abs_fft_matrix))  #This code segment simply plots the power spectrum obtained above\n",
        "plt.plot(t,abs_fft_matrix)\n",
        "plt.ylabel(‘frequency’)\n",
        "plt.xlabel(‘time’)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WCOnsbkhzYy"
      },
      "source": [
        "\n",
        "1. https://appliedmachinelearning.blog/2017/06/14/voice-gender-detection-using-gmms-a-python-primer/\n",
        "Python For Audio Signal Processing\n",
        "2. https://appliedmachinelearning.blog/2017/06/14/voice-gender-detection-using-gmms-a-python-primer/\n",
        "3. Introduction to Speech Processing https://wiki.aalto.fi/display/ITSP/Windowing\n",
        "4. Spectral leakage and windowing https://superkogito.github.io/blog/SpectralLeakageWindowing.html\n",
        "5. https://deerishi.wordpress.com/2013/09/23/signal-processing-using-python-part-1/\n",
        "https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5Og0-wt9uK"
      },
      "source": [
        "Audio Features and Feature extraction from audio signal:\n",
        "Audio signals consist of features.\n",
        "However, we need to extract the characteristics relevant to the problem we are trying to solve. Features that can be extaercted from audio signals are:\n",
        "\n",
        " 1. Spectrogram\n",
        " 2. Spectral Centroid\n",
        " 3. Spectral Rolloff\n",
        " 4. Spectral Bandwidth\n",
        " 5. Zero-Crossing Rate\n",
        " 6. Mel-Frequency Cepstral Coefficients(MFCCs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubBC9pC7uEwY"
      },
      "source": [
        "Spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rW-LZJ0t85E"
      },
      "source": [
        "X = librosa.stft(x)\n",
        "Xdb = librosa.amplitude_to_db(abs(X))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeB8WMU7uMLQ"
      },
      "source": [
        "X = librosa.stft(x)\n",
        "Xdb = librosa.amplitude_to_db(abs(X))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89RecxNxu1Ef"
      },
      "source": [
        "Spectral Centroid\n",
        "\n",
        "The spectral centroid indicates where the center of mass for a sound is located."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqSXwZJJvGls"
      },
      "source": [
        "import sklearn\n",
        "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
        "spectral_centroids.shape\n",
        "(775,)\n",
        "# Computing the time variable for visualization\n",
        "plt.figure(figsize=(12, 4))\n",
        "frames = range(len(spectral_centroids))\n",
        "t = librosa.frames_to_time(frames)\n",
        "# Normalising the spectral centroid for visualisation\n",
        "def normalize(x, axis=0):\n",
        "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
        "#Plotting the Spectral Centroid along the waveform\n",
        "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
        "plt.plot(t, normalize(spectral_centroids), color='b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4kN2LS1vVVH"
      },
      "source": [
        "Spectral Rolloff\n",
        "It depicts the frequency at which high frequencies decrease to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRcoUFzTvvXw"
      },
      "source": [
        "spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
        "plt.plot(t, normalize(spectral_rolloff), color='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPojywqYv1tX"
      },
      "source": [
        "### Spectral Bandwidth\n",
        "The spectral bandwidth is the width of the band of information at one-half the peak maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTuqY5qtwIRo"
      },
      "source": [
        "spectral_bandwidth_1 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr)[0]\n",
        "spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=3)[0]\n",
        "spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=4)[0]\n",
        "spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=5)[0]\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
        "plt.plot(t, normalize(spectral_bandwidth_1), color='b')\n",
        "plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
        "plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
        "plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
        "plt.legend(('p = 2', 'p = 3', 'p = 4'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOQXGa0gwSul"
      },
      "source": [
        "Zero-Crossing Rate\n",
        "\n",
        "A very simple way for measuring the smoothness of a signal is to calculate the number of zero-crossing within a segment of that signal. A voice signal oscillates slowly — for example, a 100 Hz signal will cross zero 100 per second — whereas an unvoiced fricative can have 3000 zero crossings per second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lic-F_aAwa-6"
      },
      "source": [
        "#Plot the signal:\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sr)\n",
        "# Zooming in\n",
        "n0 = 9000\n",
        "n1 = 9100\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(x[n0:n1])\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKVYXUpTwiPn"
      },
      "source": [
        "n0 = 9000\n",
        "n1 = 9100\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(x[n0:n1])\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqS54-8wkcN"
      },
      "source": [
        "zero_crossings = librosa.zero_crossings(x[n0:n1], pad=False)\n",
        "print(sum(zero_crossings))#16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOfBRxe8wpVp"
      },
      "source": [
        "Mel-Frequency Cepstral Coefficients(MFCCs)\n",
        "\n",
        "The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIRqdJ1Gwop6"
      },
      "source": [
        "mfccs = librosa.feature.mfcc(x, sr)\n",
        "print(mfccs.shape)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amt2lsXxw30V"
      },
      "source": [
        "Chroma feature\n",
        "A chroma feature or vector is typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, …, B}, is present in the signal. In short, It provides a robust way to describe a similarity measure between music pieces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6PiIH6Hw2e0"
      },
      "source": [
        "chromagram=librosa.feature.chroma_stft(x, sr=sr)\n",
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4a_2rPwNfy"
      },
      "source": [
        "#Plot the signal:\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sr)\n",
        "# Zooming in\n",
        "n0 = 9000\n",
        "n1 = 9100\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(x[n0:n1])\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z0L-5wJfO5A"
      },
      "source": [
        "# Data Augmentation Methods for Speech Data\n",
        "Generally speaking, any data augmentation method aims to increase the size of the dataset and provide multiple variations of each data sample. We can augment data samples if we create new samples by changing small portions in the original samples. The main advantage of using data augmentation methods is to overcome the overfitting issue and to develop generalized supervised classifiers.\n",
        "To creat new data sample, we can employ the following methods:\n",
        "Noise Injection\n",
        "Shifting Time\n",
        "Changing Pitch\n",
        "Changing Speed\n",
        "\n",
        "## Articles\n",
        "1.[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)\n",
        "## Tutorials\n",
        "i. [Data Augmentation for Speech Recognition](https://towardsdatascience.com/data-augmentation-for-speech-recognition-e7c607482e78)\n",
        "\n",
        "ii. https://dev.to/makcedward/data-augmentation-for-speech-recognition-bfc\n",
        "iii. Data Augmentation in Python: Everything You Need to Know https://neptune.ai/blog/data-augmentation-in-python\n",
        "iv. https://www.kaggle.com/CVxTz/audio-data-augmentation\n",
        "\n",
        "v. https://project-awesome.org/faroit/awesome-python-scientific-audio#data-augmentation\n",
        "\n",
        "## Github\n",
        "a. https://github.com/iver56/audiomentations\n",
        "\n",
        "b. https://github.com/SuperKogito/pydiogment\n",
        "\n",
        "c.https://muda.readthedocs.io/en/latest/\n",
        "## Libraries\n",
        "1. audiomentations https://pypi.org/project/audiomentations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuMu4EpwRY3O"
      },
      "source": [
        "#What is speech recognition?\n",
        "Speech recognition, also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, is a capability which enables a program to process human speech into a written format. While it’s commonly confused with voice recognition, speech recognition focuses on the translation of speech from a verbal format to a text one whereas voice recognition just seeks to identify an individual user’s voice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXjXpLM-T-5I"
      },
      "source": [
        "# What is speaker recognition?\n",
        "\n",
        " A Speaker recognition (e.g., voice recognition and speech recognition) system is a **speaker identification** or **speaker verification**.  A speaker recognition system aims to identify an individuals  from his characteristics of voices. On the other hand, a speaker recognition system answers the question \"***Who is speaking***?\".  A Speaker recognition system csn be prsented as *Speaker Identification* or **Speaker Verification**.\n",
        "\n",
        "A **Speaker Identification** system aims to determine from which of the registered speakers a given utterance comes.\n",
        "\n",
        "A **Speaker Verification** system can accept or reject the identity claimed by a speaker.\n",
        "\n",
        "Speaker verification can be either ***text-dependent*** or ***text-independent***.\n",
        "\n",
        "*Text-dependent *Speaker Verification means speakers need to choose the same passphrase to use during both enrollment and verification phases.\n",
        "\n",
        "*Text-independent* verification means speakers can speak in everyday language in the enrollment and verification phrases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ6DEMQR8DBO"
      },
      "source": [
        "Refrences:\n",
        "A Tutorial on Text-Independent Speaker Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QExk_4USdMcw"
      },
      "source": [
        "\n",
        "### More Python codes\n",
        "1. [Speaker_Recogniton_Verification.ipynb](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/speaker_recognition/Speaker_Recognition_Verification.ipynb)\n",
        "\n",
        "2. [Speaker Recognition](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/audio/ipynb/speaker_recognition_using_cnn.ipynb)\n",
        "\n",
        "3. [Speaker_Diarization_Inference.ipynb](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/speaker_recognition/Speaker_Diarization_Inference.ipynb#scrollTo=oNVGEzW2f0mF)\n",
        "\n",
        "4. [Identifying speakers with voice recognition](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781787125193/9/ch09lvl1sec61/identifying-speakers-with-voice-recognition)\n",
        "\n",
        "5. [Speech Recognition.ipynb](https://github.com/aravindpai/Speech-Recognition/blob/master/Speech%20Recognition.ipynb)\n",
        "\n",
        "### Tutorial\n",
        "[1]. [Speaker recognition](http://www.scholarpedia.org/article/Speaker_recognition)\n",
        "\n",
        "[2]. [A Tutorial on Text-Independent Speaker Verification](https://link.springer.com/content/pdf/10.1155/S1110865704310024.pdf)\n",
        "\n",
        "[3]. [Real-Time Speaker Identification and Verification](https://observatoriouniversidadesoaf.com/BUENAS_PRACTICAS/uoc_tesla_trust_based_authentication/bibliografia_nueva/Speaker%20Recognition.pdf)\n",
        "\n",
        "[4]. [A tutorial on speaker verification](http://cslt.riit.tsinghua.edu.cn/mediawiki/images/c/cb/131104-ivector-microsoft-wj.pdf)\n",
        "\n",
        "[5]. [An Overview of Automatic Speaker Verification System](https://link.springer.com/chapter/10.1007/978-981-10-7245-1_59)\n",
        "\n",
        "[6]. [SpeechRecognition](https://pypi.org/project/SpeechRecognition/)\n",
        "\n",
        "[7]. [Easy Speech-to-Text with Python](https://www.kdnuggets.com/2020/06/easy-speech-text-python.html)\n",
        "\n",
        "[8]. [Speech Recognition in Python— The Complete Beginner’s Guide](https://sonsuzdesign.blog/2020/08/14/speech-recognition-in-python-the-complete-beginners-guide/)\n",
        "\n",
        "[9]. [Recognition: a review of the different deep learning approaches](https://theaisummer.com/speech-recognition/)"
      ]
    }
  ]
}